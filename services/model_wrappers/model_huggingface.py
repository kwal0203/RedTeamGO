from services.model_wrappers.base_model import WrapperModel
from typing import Optional, List
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
)
from utils.config import device


class HuggingFaceModel(WrapperModel):
    """
    This class provides a wrapper for HuggingFace models from the
    'transformers' library.

    Attributes:
        name (Optional[str]): TODO.
        decription (Optional[str]): TODO.
        model (Any): TODO.
        tokenizer (Any): TODO.

    Methods:
        preprocess: TODO.
        postprocess: TODO.
        model_predict: TODO.
        _model_predict: TODO.
    """

    def __init__(
        self,
        name: Optional[str] = "my_huggingface_model",
        description: Optional[str] = "Large language model",
    ) -> None:
        """
        Initializes the HuggingFaceModel class.

        Args:
            name Optional[str]: A name for the wrapper class.
            description Optional[str]: Description of the model being wrapped.
        """
        super().__init__(name=name, description=description)

        self.model = AutoModelForCausalLM.from_pretrained(name)
        self.tokenizer = AutoTokenizer.from_pretrained(name)
        self.model.to(device)

    def preprocess(self, data: List[str]) -> List[str]:
        """
        Preprocess prompt.

        Args:
            data (List[str]): TODO.
        """
        tokens = [
            self.tokenizer(i + self.tokenizer.eos_token, return_tensors="pt").to(device)
            for i in data
        ]
        return tokens

    def postprocess(self, generated_text):
        """
        Postprocess text generated by LLM.

        Args:
            generated_text (List[str]): TODO.
        """
        return generated_text

    def model_predict(self, data: List[str]) -> List[str]:
        """
        Obtain generated responses after inputting prompts. Perform input and
        output processing if required.

        Args:
            data (List[str]): TODO.
        """
        input = self.preprocess(data=data)
        response = self._model_predict(inputs=input)
        output = self.postprocess(responses=response)
        return output

    def _model_predict(self, inputs) -> List[str]:
        """
        Helper function for obtaining generated response from input prompts.

        Args:
            inputs (List[int]): TODO.
        """
        responses = []
        for input in inputs:
            outputs = self.model.generate(
                input.input_ids,
                max_length=500,
                pad_token_id=self.tokenizer.eos_token_id,
                num_return_sequences=1,
            )
            generated_text = self.tokenizer.decode(
                outputs[0], skip_special_tokens=False
            )
            responses.append(generated_text)

        return responses
